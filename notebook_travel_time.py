# -*- coding: utf-8 -*-
"""notebook-travel-time.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fJES6o1fsQSq_66joof6wW0J-axqLGlC

# **Finding the Shortest Travel Time by using Machine Learning and Genetic Algorithm**

The purpose of this project is to find the shortest route given some locations. For that, a machine learning model is builded for predicting the travel time between each pair of locations. After that, a genetic algorithm model is used to find which sequence of locations takes the shortest time.

Also, in this approach is used both continuous and categorical features. The categorical features also include information about weather. Futhermore, embedding layers are used and they learn a set of weights for each of the categorical columns. 

Datasets:
1.   [Weather data in New York City - 2016](https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016)
2.   [New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration)
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
pd.set_option('display.max_columns', None)
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
# %matplotlib inline

plt.style.use('seaborn')

# Intall Cartopy and a few pre-requisites
!apt-get install -qq libgdal-dev libproj-dev

!apt-get -V -y -qq install python-cartopy python3-cartopy

!pip uninstall shapely -y

!pip install shapely --no-binary shapely

"""**Loading and Visualizing the Data**"""

weather = pd.read_csv("weather.csv") 
train = pd.read_csv("train.csv")[:100000]

print(train.shape)
train.head()

# Plot pickup and dropff's localizations
import cartopy.crs as ccrs
import cartopy
import cartopy.feature as cfeature
import numpy as np

extent = [-74.7, -73.1, 40.3, 41.2]

# Pickup

ax = plt.subplot(1, 2, 1)
ax = plt.axes(projection=ccrs.AlbersEqualArea())
ax.set_extent(extent)

ax.add_feature(cartopy.feature.OCEAN)
ax.add_feature(cartopy.feature.LAND, edgecolor='black')
ax.add_feature(cartopy.feature.LAKES, edgecolor='black')
ax.add_feature(cartopy.feature.RIVERS)
lon = train.pickup_longitude
lat = train.pickup_latitude


ax.plot(lon, lat, 'r.', transform=ccrs.PlateCarree(), zorder=2, label='Pickup')
ax.coastlines(resolution='10m')
ax.legend()
plt.show()

# Dropoff
ax = plt.subplot(1, 2, 2)
ax = plt.axes(projection=ccrs.AlbersEqualArea())
ax.set_extent(extent)

ax.add_feature(cartopy.feature.OCEAN)
ax.add_feature(cartopy.feature.LAND, edgecolor='black')
ax.add_feature(cartopy.feature.LAKES, edgecolor='black')
ax.add_feature(cartopy.feature.RIVERS)
lon = train.dropoff_longitude
lat = train.dropoff_latitude

ax.plot(lon, lat, 'b.', transform=ccrs.PlateCarree(), zorder=2, label='Dropoff')
ax.coastlines(resolution='10m')
ax.legend()
plt.show()

# Search nulls and percentage of them
def check_null(df):
    total = df.isnull().sum().sort_values(ascending=False)
    percent_1 = df.isnull().sum()/df.isnull().count()*100
    percent_2 = (round(percent_1, 1)).sort_values(ascending=False)
    missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])
    print(missing_data.head(5))

check_null(train)

"""**Preparing the Data**"""

# Create attributes in weather dataset
from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder = OrdinalEncoder(dtype=np.int32)

weather.Time = pd.to_datetime(weather.Time)
weather = weather[weather.Time.dt.year == 2016]

weather.Conditions = ordinal_encoder.fit_transform(np.array(weather.Conditions).reshape(-1, 1))

weather["pickup_month"] = weather.Time.dt.month
weather["pickup_day"] = weather.Time.dt.day
weather["pickup_hour"] = weather.Time.dt.hour

weather.head()

# Remove outliers in train
old_len = len(train)
train = train[(train.trip_duration >= np.percentile(train.trip_duration, 0.99)) &
              (train.trip_duration <= np.percentile(train.trip_duration, 99.99))]
print("Removed {} outliers".format(old_len - len(train)))

# Calculate distance by using picup and dropoff's latitude and longitude
from geopy import distance

train['distance']  = train.apply(lambda row: distance.geodesic((row['dropoff_latitude'],row['dropoff_longitude']), 
                                        (row['pickup_latitude'], row['pickup_longitude'])).km, axis=1)

train.head()

# Convert to datetime
train.pickup_datetime = pd.to_datetime(train.pickup_datetime)
train.dropoff_datetime = pd.to_datetime(train.dropoff_datetime)

# Generate some attributes related to time
train['pickup_minute_of_the_day'] = train.pickup_datetime.dt.hour*60 + train.pickup_datetime.dt.minute
train["pickup_month"] = train.pickup_datetime.dt.month
train["pickup_day"] = train.pickup_datetime.dt.day
train["pickup_hour"] = train.pickup_datetime.dt.hour
train['weekday'] = train.pickup_datetime.dt.weekday

train = pd.merge(train, weather[['pickup_month', 'pickup_day', 'pickup_hour','Conditions']], how = 'left', on = ['pickup_month', 'pickup_day', 'pickup_hour'])

# Remove some samples whose datetime doesn't match 
train = train.dropna(subset=["Conditions"])

train.head()

"""**Preparing sets**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(train, train['trip_duration'], test_size=0.01, random_state=42)

X_train_continuous = X_train[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'distance', 'pickup_minute_of_the_day']]
X_train_categorical = X_train[['pickup_month', 'weekday', 'Conditions']]

X_test_continuous = X_test[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'distance', 'pickup_minute_of_the_day']]
X_test_categorical = X_test[['pickup_month', 'weekday', 'Conditions']]

X_train_continuous.head()

# Feature scaling
from sklearn import preprocessing

scaler = preprocessing.StandardScaler().fit(X_train_continuous)
X_train_continuous = scaler.transform(X_train_continuous)

X_test_continuous = scaler.transform(X_test_continuous)

"""**Training the model**"""

from tensorflow.keras.optimizers import SGD
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Flatten, Embedding, Dense, Activation, Input, concatenate, Reshape, Dropout

emb_dim = 3  # dimension of the emdedding vectors

# input_dim correponds to the number of possible categories + 1
input0 = Input(shape=(1,))
model0 = Embedding(input_dim=X_train_categorical['pickup_month'].value_counts().size+1,output_dim=emb_dim)(input0)
model0 = Flatten()(model0)

input1 = Input(shape=(1,))
model1 = Embedding(input_dim=X_train_categorical['weekday'].value_counts().size+1, output_dim=emb_dim)(input1)
model1 = Flatten()(model1)

input2 = Input(shape=(1,))
model2 = Embedding(input_dim=X_train_categorical['Conditions'].value_counts().size+3, output_dim=emb_dim)(input2)
model2 = Flatten()(model2)

continuous_input = Input(shape=(X_train_continuous.shape[1], ))

conc_model = concatenate([continuous_input,model0, model1, model2])
dense_model = Dense(units=100, activation='relu')(conc_model)
dense_model = Dropout(0.25)(dense_model)
dense_model = Dense(units=100, activation='relu')(dense_model)
out_model = Dense(units=1)(dense_model)

model = Model(inputs=[continuous_input,input0, input1, input2], outputs=out_model)
sgd = SGD(lr=0.1)
model.compile(optimizer='adam', loss="mean_absolute_error", metrics=["mean_absolute_error"])

history = model.fit([X_train_continuous, np.array(X_train_categorical['pickup_month']), np.array(X_train_categorical['weekday']), np.array(X_train_categorical['Conditions'])], 
          y_train, epochs=50, validation_split = 0.2)

model.summary()

tf.keras.utils.plot_model(
    model, to_file='model.png', show_shapes=False, show_layer_names=True,
    rankdir='TB', expand_nested=False, dpi=96
)

"""**Checking the trained model**"""

# Plot loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper right')
plt.show()

# Test some samples from test dataset
df_test = pd.DataFrame(data={'trip_duration':y_test[0:10].copy()})

df_test['pred'] = model.predict([X_test_continuous[0:10], np.array(X_test_categorical['pickup_month'])[0:10], 
               np.array(X_test_categorical['weekday'])[0:10], np.array(X_test_categorical['Conditions'])[0:10]]).astype('int32')
df_test['%diff'] = 100 * (df_test['pred'] / df_test['trip_duration'])

df_test.sort_values("%diff", ascending=False)

# Calculate distance between categoricals from embedding
from scipy import spatial

keras_embs0 = model.layers[3].get_weights()[0] 
keras_embs1 = model.layers[4].get_weights()[0] 

spatial.distance.cosine(keras_embs0[2], keras_embs1[3])

"""**Finding shortest path**"""

# Install deap library to create the GA model
!pip install deap

np.random.seed(0) 
np.random.rand(0)

# Get locations 
locations_count = 8
indices = list(np.random.randint(0, X_test.shape[0], size=locations_count))
locations_coord = {}

# Get locations' coordinates 
for i in range(len(indices)):
  locations_coord[i] = [X_test.iloc[indices[i]]['pickup_longitude'], X_test.iloc[indices[i]]['pickup_latitude']] 

# Get the categorical attributes
# Those categorical attributes are fixed among the locations used in GA
trip_month = [X_test_categorical.iloc[indices[0]]['pickup_month']]
trip_weekday = [X_test_categorical.iloc[indices[0]]['weekday']]
trip_weather = [X_test_categorical.iloc[indices[0]]['Conditions']]

# Get minute of the day in which the driver starts his trip 
# by using the first location selected in random
start_minute_of_the_day = X_test.iloc[indices[0]]['pickup_minute_of_the_day']

locations_coord

# Prepare function to calculate total duration of an indivudual

def evalOneMin(individual):
    total_duration = 0
    current_minute_of_the_day = start_minute_of_the_day
    for i in range(0, len(individual)-1):
        # It should avoid repeating the pathes in individual
        if individual.count(individual[i]) > 1:
            return 99999,
            
        trip_distance = distance.geodesic((locations_coord[individual[i+1]][1], locations_coord[individual[i+1]][0]), 
                                     (locations_coord[individual[i]][1], locations_coord[individual[i]][0])).km

        continuous_sample = scaler.transform([[locations_coord[individual[i]][0], locations_coord[individual[i]][1],
                                   locations_coord[individual[i+1]][0], locations_coord[individual[i+1]][1],
                                   trip_distance, current_minute_of_the_day]])
        
        duration = model.predict([np.array(continuous_sample), np.array(trip_month), np.array(trip_weekday), np.array(trip_weather)])
        
        # current_minute_of_the_day is increased for each pair of location traveled 
        current_minute_of_the_day += int(duration/60)
        total_duration += duration
      
      
    return total_duration,

# GA initializers
import array
import random

from deap import algorithms
from deap import base
from deap import creator
from deap import tools


# Weights is -1 because I want a fitness score as low as possible
creator.create("FitnessMax", base.Fitness, weights=(-1.0,))
creator.create("Individual", array.array, typecode='b', fitness=creator.FitnessMax)

toolbox = base.Toolbox()

# Attribute generator: generates chromosomes that vary from 0 to (locations_count-1)
toolbox.register("attr_num", random.randint, 0, locations_count-1)

# Structure initializers
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_num, locations_count)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

toolbox.register("evaluate", evalOneMin)
toolbox.register("mate", tools.cxTwoPoint)
toolbox.register("mutate", tools.mutFlipBit, indpb=0.05)
toolbox.register("select", tools.selTournament, tournsize=3)

# Execute evolution 

random.seed(0)

pop = toolbox.population(n=200)
hof = tools.HallOfFame(1)
stats = tools.Statistics(lambda ind: ind.fitness.values)
stats.register("avg", np.mean)
stats.register("std", np.std)
stats.register("min", np.min)
stats.register("max", np.max)

pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.1, ngen=30, 
                                stats=stats, halloffame=hof, verbose=True)

pop, log, hof

best_ind = tools.selBest(pop, 1)[0]
print("Best individual is {} and its time is {} seg".format(list(best_ind),
             float(best_ind.fitness.values[0])))